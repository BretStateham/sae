{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and generic utilities\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import pickle\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "# Pandas, numpy, matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn - import feature engineering and classification learners\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Scikit-learn - import utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, classification_report, auc, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, log_loss, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# Optional - LightGBM\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file location\n",
    "INDIR  = './sentiment140/'\n",
    "INDATA = 'training.1600000.processed.noemoticon.csv'\n",
    "\n",
    "infile = os.path.join(INDIR, INDATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and inspect label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(infile, sep=',', header=None, encoding='ISO-8859-1')\n",
    "df.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "print(\"Dataset size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target label 4 to 1 for binary classification\n",
    "#df.target[df.target == 4] = 1\n",
    "#df.target = df.target.astype(str)\n",
    "df.loc[df.target == 4, 'target'] = 1\n",
    "\n",
    "target_cnt = Counter(df.target)\n",
    "target_cnt = {str(k):v for k, v in target_cnt.items()}\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(target_cnt.keys(), target_cnt.values())\n",
    "plt.title(\"Dataset labels distribuition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a sample for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a 10% random sample\n",
    "sample_size = 0.1\n",
    "df2 = df.sample(frac=sample_size)\n",
    "print(\"Sample size:\", len(df2))\n",
    "df2.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df2.text.sample(25).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Plot a cloud of words for negative tweets :\n",
    "data_neg = df2[df2.target == 0]['text']\n",
    "plt.figure(figsize=(20, 20))\n",
    "wc = WordCloud(max_words=1000, width=1600, height=800,\n",
    "               collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a cloud of words for positive tweets :\n",
    "data_pos = df2[df2.target == 1]['text']\n",
    "plt.figure(figsize=(20, 20))\n",
    "wc = WordCloud(max_words=1000, width=1600, height=800,\n",
    "               collocations=False).generate(\" \".join(data_pos))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to clean text\n",
    "def deaccent(text):\n",
    "    norm = unicodedata.normalize(\"NFD\", text)\n",
    "    result = ''.join(ch for ch in norm if unicodedata.category(ch) != 'Mn')\n",
    "    return unicodedata.normalize(\"NFC\", result)\n",
    "\n",
    "def clean_text(text, lowercase=True, nopunct=True, deacc=True, nomention=True):\n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Deaccent text\n",
    "    if deacc:\n",
    "        text = deaccent(text)\n",
    "              \n",
    "    # Remove mentions\n",
    "    if nomention:\n",
    "        text = re.sub('@\\w+ *', ' ', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if nopunct:\n",
    "        puncts = string.punctuation    # all special characters\n",
    "        #puncts = string.punctuation.replace(\"'\", \"\")   # keep apostrophe\n",
    "        for c in puncts:\n",
    "            text = text.replace(c, ' ')\n",
    "        \n",
    "    # Remove newlines - Compact and strip whitespaces\n",
    "    text = re.sub('[\\\\r\\\\n]+', ' ', text)\n",
    "    text = re.sub('\\\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"@nny24 Yeah!!! ^^ I got good news... but not GREAT  I hope to tell you the great news tonight @any123_456 =D. And... I'm sooo motivated girl... @whatever123\"\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and clean before training/testing\n",
    "x_all = df2.text\n",
    "y_all = df2.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=42)\n",
    "x_train_clean = x_train.apply(lambda x: clean_text(x))\n",
    "x_test_clean  = x_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization: Explore word ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_features(vectorizer, documents, n_top=10):\n",
    "    tfidf = vectorizer.fit_transform(documents)\n",
    "    print('# of ngrams = %d\\n' % len(vectorizer.vocabulary_))\n",
    "    \n",
    "    importance = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    print('Most frequent ngrams in the vocabulary:')\n",
    "    pprint(feature_names[importance[:n_top]])\n",
    "    print()\n",
    "    print('Least frequent ngrams in the vocabulary:')\n",
    "    pprint(feature_names[importance[-n_top:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vect = TfidfVectorizer(ngram_range=(1, 1), analyzer=\"word\", binary=False, sublinear_tf=False, \n",
    "                            min_df=3, max_df=1.0, stop_words=None)\n",
    "explore_features(word_vect, x_train, n_top=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vect = TfidfVectorizer(ngram_range=(1, 1), analyzer=\"word\", binary=False, sublinear_tf=False, \n",
    "                            min_df=3, max_df=1.0, stop_words='english',\n",
    "                            strip_accents='ascii', lowercase=False)\n",
    "explore_features(word_vect, x_train, n_top=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vect = TfidfVectorizer(ngram_range=(1, 1), analyzer=\"word\", binary=False, sublinear_tf=False, \n",
    "                            min_df=3, max_df=1.0, stop_words='english',\n",
    "                            strip_accents='ascii', lowercase=True)\n",
    "explore_features(word_vect, x_train, n_top=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization: TFIDF using word and character ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together in a pipeline- Use both word and character ngram features\n",
    "word_vect = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"word\", binary=False, sublinear_tf=False, \n",
    "                            min_df=3, max_df=1.0, stop_words='english',\n",
    "                            #strip_accents='ascii', lowercase=True\n",
    "                           )\n",
    "char_vect = TfidfVectorizer(ngram_range=(1, 3), analyzer=\"char_wb\", binary=False, sublinear_tf=False,\n",
    "                            #strip_accents='ascii', lowercase=True\n",
    "                           )\n",
    "combined_vect = FeatureUnion([(\"word\", word_vect), (\"char\", char_vect)])\n",
    "\n",
    "print(\"Extracting features from the training data using a word+char ngrams vectorizer\")\n",
    "t0 = time()\n",
    "X_train = combined_vect.fit_transform(x_train_clean)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs\" % duration)\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = combined_vect.transform(x_test_clean)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs\" % duration)\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the combined vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore word ngrams\n",
    "print('# of word ngrams = %d' % len(word_vect.vocabulary_))\n",
    "#sorted_word_vect = sorted(word_vect.vocabulary_.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "pprint(list(word_vect.vocabulary_.items())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore character ngrams\n",
    "print('# of character ngrams = %d' % len(char_vect.vocabulary_))\n",
    "#sorted_char_vect = sorted(char_vect.vocabulary_.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pprint(list(char_vect.vocabulary_.items())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, evaluation and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clf, train_x, train_y):\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(train_x, train_y)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "    return\n",
    "\n",
    "def evaluate(model, test_x, y_true):\n",
    "    t1     = time()\n",
    "    y_pred = model.predict(test_x)\n",
    "    t2     = time()\n",
    "    duration = t2 - t1\n",
    "    print('Evaluation time for %d comments = %.6f secs --> %.6f sec/tweet\\n' % \n",
    "                                (len(y_pred), duration, duration/len(y_pred)))\n",
    "\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    print('Performance metrics:')\n",
    "    print('Accuracy   = %.6f' % accuracy_score(y_true, y_pred))\n",
    "    print('AUC        = %.6f' % roc_auc_score(y_true, y_pred))\n",
    "    print('Log-loss   = %.6f' % log_loss(y_true, y_pred))\n",
    "    print('Precision  = %.6f' % precision_score(y_true, y_pred))\n",
    "    print('Recall     = %.6f' % recall_score(y_true, y_pred))\n",
    "    print('F1-Score   = %.6f' % f1_score(y_true, y_pred))\n",
    "\n",
    "    print()\n",
    "    print('Confusion matrix:')\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=model.classes_)\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    cm_disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    print('ROC Curve:')\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_disp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                  estimator_name=type(model).__name__)\n",
    "    roc_disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    print('Precision-Recall curve:')\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    pr_disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "    pr_disp.plot()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def test_predict(model, text, prob=True):\n",
    "    print('%s\\n' % text)\n",
    "    text = clean_text(text)\n",
    "    tst = combined_vect.transform([text])\n",
    "    preds = model.predict(tst)\n",
    "    print('Predicted: ' + str(preds[0]))\n",
    "    if prob:\n",
    "        preds_proba = model.predict_proba(tst)\n",
    "        print('Probas   : ' + str(preds_proba[0]))\n",
    "    return\n",
    "\n",
    "def test_byind(model, ind=None, prob=True):\n",
    "    if ind is None:\n",
    "        ind = int(random.random() * len(x_test))\n",
    "    print('Test index: %d' % ind)\n",
    "    tst = x_test.values[ind]\n",
    "    lbl = y_test.values[ind]\n",
    "    test_predict(model, tst, prob=prob)\n",
    "    print('Truth    : %d' % lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate various learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a LogisticRegression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(penalty='l2', dual=False, tol=0.000001, C=10.0, fit_intercept=True, \n",
    "                         intercept_scaling=1, class_weight=None, random_state=None, \n",
    "                         solver='liblinear', max_iter=10000, verbose=1, warm_start=False, n_jobs=1)\n",
    "\n",
    "train(lgr, X_train, y_train)\n",
    "evaluate(lgr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Linear SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm = LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
    "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
    "     verbose=1)\n",
    "\n",
    "train(svm, X_train, y_train)\n",
    "evaluate(svm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a RandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random forest training is very slow in this implementation\n",
    "# rf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, \n",
    "#                        min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "# train(rf, X_train, y_train)\n",
    "# evaluate(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a LightGBM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm = lgb.LGBMClassifier(objective='binary', \n",
    "                        num_leaves=31, \n",
    "                        learning_rate=0.05, \n",
    "                        n_estimators=100) \n",
    "\n",
    "\n",
    "train(gbm, X_train, y_train)\n",
    "evaluate(gbm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a single tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict(svm, 'I am so happy today', prob=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict(lgr, 'I am so happy today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict(gbm, 'I am so happy today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict(lgr, 'I am so miserable today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_byind(lgr, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test random tweets from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some random test cases\n",
    "for i in range(20):\n",
    "    test_byind(lgr)\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = {\n",
    "    0: [1, \"@nny24 Yeah!!! ^^ I got good news... but not GREAT  I hope to tell you the great news tonight =D. And... I'm sooo motivated girl... \"],\n",
    "    1: [1, \"@nissalomax hey nissa!  I'm ok...not GREAT...but not bad either \"],\n",
    "    2: [0, \"I'm not happy\"],\n",
    "    3: [0, \"I'm not happy at all\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tst.keys():\n",
    "    print('Truth    : %d' % tst[i][0])\n",
    "    test_predict(lgr, tst[i][1], prob=True)\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
